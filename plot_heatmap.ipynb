{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "def download(name, model, iters):\n",
    "    trained_policy = '{}-{}.iter={}.npz'.format(name, model, iters)\n",
    "    if not os.path.exists('.policy/{}'.format(trained_policy)):\n",
    "        print 'download the model from HPC server'\n",
    "        os.system('scp mercer:/scratch/jg5223/exp/.policy/{} .policy/'.format(trained_policy))\n",
    "    print 'done'\n",
    "# m ='model_wmt15_bpe2k_uni_en-ru_rev.npz'    \n",
    "# download('160920-225547', m, '44400')\n",
    "# download('160920-225339', m, '33600') \n",
    "# download('160920-230514', m, '95600')\n",
    "# download('160920-225917', m, '68000')\n",
    "# download('160920-230503', m, '48200')\n",
    "# download('160920-225454', m, '45200')\n",
    "\n",
    "# m ='model_wmt15_bpe2k_uni_en-de_rev.npz' \n",
    "# download('160925-000643', m, '24400')\n",
    "# download('160925-000657', m, '10400')\n",
    "# download('160925-000642', m, '8000') \n",
    "# download('160925-000735', m, '22200')\n",
    "# download('160925-000751', m, '26200')\n",
    "# download('160925-000811', m, '18800')\n",
    "\n",
    "# max model 172 160917-175503 Iter=34400 AP=0.3 0.14121125914\n",
    "# max model 262 160917-001117 Iter=52400 AP=0.5 0.157379802964\n",
    "# max model 141 160917-175423 Iter=28200 AP=0.7 0.15857511208\n",
    "# max model 67 160917-175657 Iter=13400 CW=5.0 0.160038686998\n",
    "m ='model_wmt15_bpe2k_uni_en-de.npz' \n",
    "download('160917-175503', m, '34400')\n",
    "download('160917-001117', m, '52400')\n",
    "download('160917-175423', m, '28200') \n",
    "download('160917-175657', m, '13400')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load weaver_eval.py\n",
    "\"\"\"\n",
    "Simultaneous Machine Translateion: Training with Policy Gradient\n",
    "\n",
    "\"\"\"\n",
    "import argparse\n",
    "import os\n",
    "import cPickle as pkl\n",
    "\n",
    "from bleu import *\n",
    "from nmt_uni import *\n",
    "from policy import Controller as Policy\n",
    "from utils import Progbar, Monitor\n",
    "\n",
    "from simultrans_beam import simultaneous_decoding as simultaneous_beamsearch\n",
    "from simultrans_model import simultaneous_decoding\n",
    "from simultrans_model import _seqs2words, _bpe2words, _action2delay, PIPE, _padding\n",
    "\n",
    "import time\n",
    "\n",
    "numpy.random.seed(19920206)\n",
    "timer = time.time\n",
    "\n",
    "WORK  = '/misc/kcgscratch1/ChoGroup/thoma_exp/SimulTrans/'\n",
    "EXP   = WORK\n",
    "\n",
    "# check hidden folders\n",
    "def check_env():\n",
    "    import os\n",
    "    paths = ['.policy', '.pretrained', '.log', '.config', '.images', '.translate']\n",
    "    for p in paths:\n",
    "        p = WORK + p\n",
    "        if not os.path.exists(p):\n",
    "            os.mkdir(p)\n",
    "\n",
    "policy = OrderedDict()\n",
    "policy['prop']      = 0.5\n",
    "policy['recurrent'] = True\n",
    "policy['layernorm'] = False\n",
    "policy['updater']   = 'REINFORCE'\n",
    "policy['act_mask']  = True\n",
    "\n",
    "config = OrderedDict()\n",
    "config['step']      = 1\n",
    "config['peek']      = 1\n",
    "config['s0']        = 1\n",
    "config['sample']    = 1\n",
    "config['batchsize'] = 1\n",
    "config['target']    = 1\n",
    "config['gamma']     = 10\n",
    "config['Rtype']     = 10\n",
    "config['forget']    = False\n",
    "config['maxsrc']    = 5\n",
    "config['pre']       = False\n",
    "config['coverage']  = True\n",
    "config['upper']     = False\n",
    "config['finetune']  = 'nope'\n",
    "            \n",
    "model        = 'model_wmt15_bpe2k_uni_en-de_rev.npz'\n",
    "options_file = None\n",
    "id           = None\n",
    "check_env()\n",
    "\n",
    "if id is not None:\n",
    "    fcon = WORK + '.config/{}.conf'.format(id)\n",
    "    if os.path.exists(fcon):\n",
    "        print 'load config files'\n",
    "        policy, config = pkl.load(open(fcon, 'r'))\n",
    "\n",
    "# ============================================================================== #\n",
    "# load model model_options\n",
    "# ============================================================================== #\n",
    "_model = model\n",
    "model = WORK + '.pretrained/{}'.format(model)\n",
    "\n",
    "if options_file is not None:\n",
    "    with open(options_file, 'rb') as f:\n",
    "        options = pkl.load(f)\n",
    "else:\n",
    "    with open('%s.pkl' % model, 'rb') as f:\n",
    "        options = pkl.load(f)\n",
    "\n",
    "print 'load options...'\n",
    "for w, p in sorted(options.items(), key=lambda x: x[0]):\n",
    "    print '{}: {}'.format(w, p)\n",
    "\n",
    "print '***' * 10\n",
    "print 'load policy options...'\n",
    "for w, p in sorted(policy.items(), key=lambda x: x[0]):\n",
    "    print '{}: {}'.format(w, p)    \n",
    "\n",
    "print '***' * 10\n",
    "print 'load learning configs...'\n",
    "for w, p in sorted(config.items(), key=lambda x: x[0]):\n",
    "    print '{}: {}'.format(w, p)    \n",
    "\n",
    "# load detail settings from option file:\n",
    "dictionary, dictionary_target = options['dictionaries']\n",
    "\n",
    "def _iter(fname):\n",
    "    with open(fname, 'r') as f:\n",
    "        for line in f:\n",
    "            words = line.strip().split()\n",
    "            x = map(lambda w: word_dict[w] if w in word_dict else 1, words)\n",
    "            x = map(lambda ii: ii if ii < options['n_words'] else 1, x)\n",
    "            x += [0]\n",
    "            yield x\n",
    "\n",
    "def _check_length(fname):\n",
    "    f = open(fname, 'r')\n",
    "    count = 0\n",
    "    for _ in f:\n",
    "        count += 1\n",
    "    f.close()\n",
    "\n",
    "    return count\n",
    "\n",
    "# load source dictionary and invert\n",
    "with open(dictionary, 'rb') as f:\n",
    "    word_dict = pkl.load(f)\n",
    "word_idict = dict()\n",
    "for kk, vv in word_dict.iteritems():\n",
    "    word_idict[vv] = kk\n",
    "word_idict[0] = '<eos>'\n",
    "word_idict[1] = 'UNK'\n",
    "\n",
    "# load target dictionary and invert\n",
    "with open(dictionary_target, 'rb') as f:\n",
    "    word_dict_trg = pkl.load(f)\n",
    "word_idict_trg = dict()\n",
    "for kk, vv in word_dict_trg.iteritems():\n",
    "    word_idict_trg[vv] = kk\n",
    "word_idict_trg[0] = '<eos>'\n",
    "word_idict_trg[1] = 'UNK'\n",
    "\n",
    "## use additional input for the policy network\n",
    "options['pre'] = config['pre']\n",
    "print 'setting done.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ================================================================================= #\n",
    "# Build a Simultaneous Translator\n",
    "# ================================================================================= #\n",
    "\n",
    "# allocate model parameters\n",
    "params  = init_params(options)\n",
    "params  = load_params(model, params)\n",
    "tparams = init_tparams(params)\n",
    "\n",
    "# print 'build the model for computing cost (full source sentence).'\n",
    "trng, use_noise, \\\n",
    "_x, _x_mask, _y, _y_mask, \\\n",
    "opt_ret, \\\n",
    "cost, f_cost = build_model(tparams, options)\n",
    "print 'done'\n",
    "\n",
    "# functions for sampler\n",
    "f_sim_ctx, f_sim_init, f_sim_next = build_simultaneous_sampler(tparams, options, trng)\n",
    "\n",
    "# check the ID:\n",
    "policy['base'] = _model\n",
    "_policy = Policy(trng, options, policy, config,\n",
    "                 n_in=options['readout_dim'] + 1 if config['coverage'] else options['readout_dim'],\n",
    "                 n_out=3 if config['forget'] else 2,\n",
    "                 recurrent=policy['recurrent'], id=id)\n",
    "\n",
    "print 'build everything ok.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# make the dataset ready for training & validation\n",
    "trainIter = TextIterator(options['datasets'][0], options['datasets'][1],\n",
    "                         options['dictionaries'][0], options['dictionaries'][1],\n",
    "                         n_words_source=options['n_words_src'], n_words_target=options['n_words'],\n",
    "                         batch_size=config['batchsize'],\n",
    "                         maxlen=options['maxlen'])\n",
    "train_num = trainIter.num\n",
    "\n",
    "valid_b   = 1\n",
    "validIter = TextIterator(options['valid_datasets'][0], options['valid_datasets'][1],\n",
    "                         options['dictionaries'][0], options['dictionaries'][1],\n",
    "                         n_words_source=options['n_words_src'], n_words_target=options['n_words'],\n",
    "                         batch_size=valid_b, cache=1,\n",
    "                         maxlen=1000000)\n",
    "\n",
    "valid_num = validIter.num\n",
    "\n",
    "valid_ = options['valid_datasets'][0]\n",
    "valid_num = _check_length(valid_)\n",
    "print 'training set {} lines / validation set {} lines'.format(train_num, valid_num)\n",
    "print 'use the reward function {}'.format(chr(config['Rtype'] + 65))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the pretrained policy\n",
    "# modelname = '160921-195056'\n",
    "# iters     = '47000'\n",
    "# modelname = '160914-140602'\n",
    "# iters     = '52800'\n",
    "# modelname = '160921-195108'\n",
    "# iters     = '34000'\n",
    "# modelname = '160916-232539'\n",
    "# iters     = '61400'\n",
    "# modelname = '160921-195220'\n",
    "# iters     = 34600\n",
    "# modelname = '160916-232619'\n",
    "# iters     = '105400'\n",
    "modelname   = '160925-000643'\n",
    "iters       = '24400'\n",
    "\n",
    "trained_policy = '{}-{}.iter={}.npz'.format(modelname, _model, iters)\n",
    "if not os.path.exists('.policy/{}'.format(trained_policy)):\n",
    "    print 'download the model from HPC server'                                                                                       \n",
    "    os.system('scp jg5223@mercer:/scratch/jg5223/exp/.policy/{} .policy/'.format(trained_policy)) \n",
    "\n",
    "\n",
    "r = numpy.load('.policy/' + trained_policy)\n",
    "for p in _policy.tparams:\n",
    "    _policy.tparams[p].set_value(r[p])\n",
    "for p in _policy.tparams_b:\n",
    "    _policy.tparams_b[p].set_value(r[p])\n",
    "   \n",
    "print 'load trained policy ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # load the pretrained policy\n",
    "# modelname = '160921-195056'\n",
    "# iters     = '47000'\n",
    "# modelname = '160914-140602'\n",
    "# iters     = '52800'\n",
    "# modelname = '160921-195108'\n",
    "# iters     = '34000'\n",
    "# modelname = '160916-232539'\n",
    "# iters     = '61400'\n",
    "# modelname = '160921-195220'\n",
    "# iters     = 34600\n",
    "# # modelname = '160916-232619'\n",
    "# # iters     = '105400'\n",
    "\n",
    "# trained_policy = '.policy/{}-{}.iter={}.npz'.format(modelname, _model, iters)\n",
    "\n",
    "# r = numpy.load(trained_policy)\n",
    "# for p in _policy.tparams:\n",
    "#     _policy.tparams[p].set_value(r[p])\n",
    "# for p in _policy.tparams_b:\n",
    "#     _policy.tparams_b[p].set_value(r[p])\n",
    "   \n",
    "# print 'load trained policy ok'\n",
    "\n",
    "from insepection import *\n",
    "import shutil\n",
    "\n",
    "# ================================================================================= #\n",
    "# Main Loop: Run\n",
    "# ================================================================================= #\n",
    "def _translate(src, trg, train=False, samples=config['sample'], greedy=False):\n",
    "    ret = simultaneous_decoding(\n",
    "        f_sim_ctx, f_sim_init,\n",
    "        f_sim_next, f_cost,\n",
    "        _policy,\n",
    "        src, trg, word_idict_trg,\n",
    "        step=config['step'], peek=config['peek'], sidx=config['s0'],\n",
    "        n_samples=samples,\n",
    "        reward_config={'target': config['target'],\n",
    "                       'gamma':  config['gamma'],\n",
    "                       'Rtype':  config['Rtype'],\n",
    "                       'maxsrc': config['maxsrc'],\n",
    "                       'greedy': greedy,\n",
    "                       'upper':  config['upper']},\n",
    "        train=train,\n",
    "        use_forget=config['forget'],\n",
    "        use_newinput=config['pre'],\n",
    "        use_coverage=config['coverage'],\n",
    "        on_groundtruth=0 if config['finetune'] == 'nope' else 10, \n",
    "        full_attention=True)\n",
    "    return ret\n",
    "\n",
    "print 'Start Simultaneous Translator...'\n",
    "\n",
    "tid           = '_greedy_{}'.format(modelname)\n",
    "action_space  = ['W', 'C', 'F']\n",
    "Log_avg       = {}\n",
    "time0         = timer()\n",
    "pipe          = PIPE(['x', 'x_mask', 'y', 'y_mask', 'c_mask'])\n",
    "\n",
    "# for validation\n",
    "# doing the whole validation!!\n",
    "reference  = []\n",
    "system     = []\n",
    "refbpe     = []\n",
    "srcbpe     = []\n",
    "actions    = []\n",
    "\n",
    "print 'start validation'\n",
    "\n",
    "collections = [[], [], [], [], []]\n",
    "probar_v = Progbar(valid_num / valid_b + 1)\n",
    "validIter.reset()\n",
    "for ij, (srcs, trgs) in enumerate(validIter):\n",
    "\n",
    "    statistics = _translate(srcs, trgs, train=False, samples=1, greedy=True)\n",
    "\n",
    "    src = [w.replace('%', '$%$') + ' $<eos>$' for w in _seqs2words(statistics['SWord'], word_idict)]\n",
    "    trs = [w.replace('%', '$%$') for w in statistics['Words']]\n",
    "\n",
    "    act = statistics['action']\n",
    "    att = statistics['attentions']\n",
    "    ott = statistics['old_attend']\n",
    "    \n",
    "    # print len(src[0]), len(trs[0]), len(act[0]), len(att[0]), len(ott[0])  \n",
    "    quality, delay, reward = zip(*statistics['track']) \n",
    "    \n",
    "    reference += statistics['Ref']\n",
    "    system    += statistics['Sys']\n",
    "    \n",
    "    #print statistics['SWord']\n",
    "    srcbpe    += _seqs2words(statistics['SWord'], word_idict)\n",
    "    refbpe    += statistics['Words']\n",
    "    actions   += statistics['action']\n",
    "      \n",
    "    # compute the average consective waiting length\n",
    "    def _consective(action):\n",
    "        waits = []\n",
    "        temp = 0\n",
    "        for a in action:\n",
    "            if a == 0:\n",
    "                temp += 1\n",
    "            elif temp > 0:\n",
    "                waits += [temp]\n",
    "                temp = 0\n",
    "\n",
    "        if temp > 0:\n",
    "            waits += [temp]\n",
    "\n",
    "        mean = numpy.mean(waits)\n",
    "        gec = numpy.max(waits)  # numpy.prod(waits) ** (1./len(waits))\n",
    "        return mean, gec\n",
    "\n",
    "    def _max_length(action):\n",
    "        _cur = 0\n",
    "        _end = 0\n",
    "        _max = 0\n",
    "        for it, a in enumerate(action):\n",
    "            if a == 0:\n",
    "                _cur += 1\n",
    "            elif a == 2:\n",
    "                _end += 1\n",
    "\n",
    "            temp = _cur - _end\n",
    "            if temp > _max:\n",
    "                _max = temp\n",
    "        return _max\n",
    "\n",
    "    maxlen = [_max_length(action) for action in statistics['action']]\n",
    "    means, gecs = zip(*(_consective(action) for action in statistics['action']))\n",
    "\n",
    "    collections[0] += quality\n",
    "    collections[1] += delay\n",
    "    collections[2] += means\n",
    "    collections[3] += gecs\n",
    "    collections[4] += maxlen\n",
    "\n",
    "    #print '\\n'\n",
    "    #print src\n",
    "    #print trs\n",
    "    infoo = 'ID={}_(BLEU={}_AP={}_CW={})'.format(ij, numpy.mean(quality), numpy.mean(delay), numpy.mean(means))\n",
    "    try:\n",
    "        heatmap2(src, None, trs, act, 0, att, ott, name=modelname, info={'index': infoo})\n",
    "        shutil.copy('.images/M_{}/Idx={}||.pdf'.format(modelname, infoo), 'heatmaps/M_{}/'.format(modelname))\n",
    "    except Exception:\n",
    "        print 'i am sorry, cannot plot.'\n",
    "        \n",
    "        \n",
    "    print '\\n'\n",
    "    \n",
    "    \n",
    "    values = [('quality', numpy.mean(quality)), ('delay', numpy.mean(delay)),\n",
    "              ('wait_mean', numpy.mean(means)), ('wait_max', numpy.mean(gecs)),\n",
    "              ('max_len', numpy.mean(maxlen))]\n",
    "    probar_v.update(ij + 1, values=values)\n",
    "\n",
    "\n",
    "validIter.reset()\n",
    "valid_bleu, valid_delay, valid_wait, valid_wait_gec, valid_mx = [numpy.mean(a) for a in collections]\n",
    "print 'AVG BLEU = {}, DELAY = {}, WAIT(MEAN) = {}, WAIT(MAX) = {}, MaxLen={}'.format(\n",
    "    valid_bleu, valid_delay, valid_wait, valid_wait_gec, valid_mx)\n",
    "\n",
    "print len(system), len(reference)\n",
    "print 'Compute the Corpus BLEU={} (greedy)'.format(corpus_bleu(reference, system))\n",
    "\n",
    "\n",
    "import sys; sys.exit()\n",
    "\n",
    "# with open(WORK + '.translate/test-{}.txt'.format(tid), 'w') as fout:\n",
    "#     for sys in system:\n",
    "#         fout.write('{}\\n'.format(' '.join(sys)))\n",
    "\n",
    "# with open(WORK + '.translate/ref-{}.txt'.format(tid), 'w') as fout:\n",
    "#     for ref in reference:\n",
    "#         fout.write('{}\\n'.format(' '.join(ref[0])))\n",
    "\n",
    "with open(WORK + 'translate/analysis-{}.txt'.format(tid), 'w') as fout:\n",
    "    for iddd, (src, ref, acts) in enumerate(zip(srcbpe, refbpe, actions)):\n",
    "        fout.write(src + '\\n')\n",
    "        fout.write(ref + '\\n')\n",
    "        fout.write(' '.join([str(a) for a in acts]) + '\\n')\n",
    "        \n",
    "        q, d, m = collections[0][iddd], collections[1][iddd], collections[2][iddd] \n",
    "        fout.write('{} {} {}\\n'.format(q, d, m))\n",
    "    \n",
    "print 'write down'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# here we need to write a srcipt to compare translation results of different\n",
    "# targets\n",
    "import os\n",
    "import numpy\n",
    "import sys\n",
    "from utils import Progbar\n",
    "\n",
    "data  = dict()\n",
    "trans = dict()\n",
    "trans['source'] = []\n",
    "for parent, folders, files in os.walk('translate'):\n",
    "    for file in files:\n",
    "        if   '140602' in file:\n",
    "            name = 'ap3'\n",
    "        elif '195108' in file:\n",
    "            name = 'ap5'\n",
    "        elif '195056' in file:\n",
    "            name = 'ap7'\n",
    "        elif '232539' in file:\n",
    "            name = 'cw2'\n",
    "        elif '195220' in file:\n",
    "            name = 'cw5'\n",
    "        elif '232619' in file:\n",
    "            name = 'cw8'\n",
    "        data[name] = dict()\n",
    "        \n",
    "        with open(os.path.join(parent, file)) as f:\n",
    "            src = f.readline().strip()\n",
    "            while src:\n",
    "                if src not in trans['source']:\n",
    "                    trans['source'].append(src)\n",
    "                    \n",
    "                trg  = f.readline().strip()\n",
    "                act  = f.readline().strip()\n",
    "                scc  = f.readline().strip()\n",
    "                data[name][src] = [trg, act, scc]\n",
    "                \n",
    "                src  = f.readline().strip()\n",
    "names = []\n",
    "for name in data:\n",
    "    trans[name] = []\n",
    "    names.append(name)\n",
    "    \n",
    "for src in trans['source']:\n",
    "    for name in data:\n",
    "        trans[name].append(data[name][src])\n",
    "                  \n",
    "print 'done.'                \n",
    "\n",
    "results = open('compare21.txt', 'w')\n",
    "probar  = Progbar(len(trans['source']))\n",
    "\n",
    "for itt, src in enumerate(trans['source']):\n",
    "    srcs = src.split() + ['<eos>']\n",
    "    decs = [{name: [] for name in names} for _ in srcs]\n",
    "    \n",
    "    show = ''\n",
    "    bleu = []\n",
    "    for name in data:\n",
    "        trgs, acts, scores = [w.split() for w in trans[name][itt]]\n",
    "        trgs += ['<eos>']\n",
    "        sid = -1\n",
    "        tid = 0\n",
    "        for a in acts:\n",
    "            if a == '0':\n",
    "                sid += 1\n",
    "            if a == '1':\n",
    "                decs[sid][name].append(trgs[tid])\n",
    "                tid += 1\n",
    "        show += '{} (B {:.4}/A {:.4}/C {:.4}) ||'.format(name, scores[0], scores[1], scores[2])\n",
    "        bleu += [float(scores[0])]\n",
    "        \n",
    "    if numpy.max(bleu) < 0.5:\n",
    "        continue\n",
    "    if numpy.min(bleu) < 0.3:\n",
    "        continue\n",
    "    if not((bleu[1] < bleu[5]) and (bleu[5] < bleu[4])):\n",
    "        continue\n",
    "        \n",
    "    show += '\\n'\n",
    "    show += 'source ||\\t'\n",
    "    for name in names:\n",
    "        show += (name + '|\\t')\n",
    "    show += '\\n'\n",
    "    \n",
    "    for iss, s in enumerate(srcs):\n",
    "        show += (s + '||\\t')\n",
    "        for name in names:\n",
    "            ts = ' '.join(decs[iss][name])\n",
    "            show += (ts + '|\\t')\n",
    "        show += '\\n'\n",
    "    show += '**********' * 10 + '\\n'\n",
    "    results.write(show + '\\n')\n",
    "    probar.update(itt)\n",
    "\n",
    "\n",
    "results.close()\n",
    "print '\\ndone'   \n",
    "    \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python X",
   "language": "python",
   "name": "pythonx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
